{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "def set_increment(total_count, increment):\n",
    "    \"\"\"\n",
    "    Calculates an increment count by dividing the total count by the increment percentage.\n",
    "\n",
    "    :param total_count: The total count to calculate an increment for.\n",
    "    :param increment: The increment percentage to calculate.\n",
    "    :return: Returns an integer representing the increment count.\n",
    "    \"\"\"\n",
    "    return round(total_count / (100 / increment))\n",
    "\n",
    "def update_progress(progress_update, current_count, total_count, increment):\n",
    "    \"\"\"\n",
    "    Prints a progress update for an iterating loop if the increment threshold has been passed.\n",
    "\n",
    "    :param progress_update: The string describing the progress update.\n",
    "    :param current_count: The number of features currently iterated through.\n",
    "    :param total_count: The total number of features to iterate through.\n",
    "    :param increment: The number of iterated features at which to print a progress update.\n",
    "    \"\"\"\n",
    "    if current_count % increment == 0:\n",
    "        print(f\"{progress_update}: {current_count / total_count:.0%} complete. {current_count} records processed.\")\n",
    "\n",
    "def runtime(progress_update, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Prints the runtime for a given progress update.\n",
    "\n",
    "    :param progress_update: The string describing the progress update.\n",
    "    :param start_time: The start time of the progress update.\n",
    "    :param end_time: The end time of the progress update.\n",
    "    \"\"\"\n",
    "    print(f\"{progress_update} runtime: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling website: https://pe.usps.com/text/pub28/28apc_002.htm\n"
     ]
    }
   ],
   "source": [
    "### Pull website with suffix data\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Suffix data website information\n",
    "suffix_url = \"https://pe.usps.com/text/pub28/28apc_002.htm\"\n",
    "table_id = \"ep533076\" # suffix element's id\n",
    "\n",
    "# Pull website using Firefox WebDriver\n",
    "print(f\"Pulling website: {suffix_url}\")\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Firefox(options=options)\n",
    "driver.get(suffix_url)\n",
    "\n",
    "# Find suffix elements\n",
    "suffix_rows = driver.find_elements(By.XPATH, f\"//table[@id='{table_id}']/tbody/tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing suffixes: 0% complete. 0 records processed.\n",
      "Parsing suffixes: 10% complete. 50 records processed.\n",
      "Parsing suffixes: 20% complete. 100 records processed.\n",
      "Parsing suffixes: 30% complete. 150 records processed.\n",
      "Parsing suffixes: 40% complete. 200 records processed.\n",
      "Parsing suffixes: 50% complete. 250 records processed.\n",
      "Parsing suffixes: 60% complete. 300 records processed.\n",
      "Parsing suffixes: 70% complete. 350 records processed.\n",
      "Parsing suffixes: 80% complete. 400 records processed.\n",
      "Parsing suffixes: 90% complete. 450 records processed.\n",
      "Parsing suffixes: 100% complete. 500 records processed.\n",
      "Parsing suffixes: 100% complete. 502 records processed.\n",
      "Parsing suffixes runtime: 0:00:07.110649\n",
      "Writing suffixes to file: data sources/suffixes.csv\n"
     ]
    }
   ],
   "source": [
    "### Parse suffix elements from website\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Store table headers\n",
    "header_cols = suffix_rows[0].find_elements(By.TAG_NAME, \"td\") # find table cells\n",
    "headers = []\n",
    "for header in header_cols[:2][::-1]: # flip header order\n",
    "    header = header.text.replace(\"\\n\", \" \")\n",
    "    headers.append(header)\n",
    "\n",
    "# Identify primary street suffix names and corresponding abbreviations\n",
    "suffixes = {} # suffix data container - {abbreviation: suffix}\n",
    "full_suffix = \"\" # primary street suffix name\n",
    "\n",
    "suffix_count = len(suffix_rows) - 1 # excluding table header\n",
    "increment = set_increment(suffix_count, 10)\n",
    "progress_update = \"Parsing suffixes\"\n",
    "start_time = datetime.now()\n",
    "\n",
    "for count, row in enumerate(suffix_rows[1:]): # excluding table header\n",
    "    cols = row.find_elements(By.TAG_NAME, \"td\") # find table cells\n",
    "    if len(cols) > 1: # if multi-col row, col[0] is suffix, and col[1] is abbreviation\n",
    "        full_suffix = cols[0].text\n",
    "        suffixes[cols[1].text] = full_suffix\n",
    "    else: # if single-col row, col[0] is abbreviation, and suffix is implied\n",
    "        suffixes[cols[0].text] = full_suffix\n",
    "    \n",
    "    update_progress(progress_update, count, suffix_count, increment)\n",
    "\n",
    "update_progress(progress_update, suffix_count, suffix_count, 1)\n",
    "end_time = datetime.now()\n",
    "runtime(progress_update, start_time, end_time)\n",
    "\n",
    "# Write processed data to file\n",
    "suffixes_filepath = \"data sources/suffixes.csv\"\n",
    "print(f\"Writing suffixes to file: {suffixes_filepath}\")\n",
    "suffixes_pd = pd.DataFrame(list(suffixes.items()), columns=headers)\n",
    "suffixes_pd.to_csv(suffixes_filepath, index=False)\n",
    "\n",
    "# Close WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to server: ftp2.census.gov/geo/tiger/TIGER2024/ROADS\n",
      "230-Server: ftp2.census.gov\n",
      "230-\n",
      "230-Personal Identifiable Information (PII) shall not be placed on the FTP\n",
      "230-server without prior special arrangement and in conjunction with ITSO.\n",
      "230-\n",
      "230-NOTE: The data available for anonymous FTP download on this FTP server are\n",
      "230-also available over the Web:\n",
      "230-http://www2.census.gov\n",
      "230 Login successful.\n",
      "Downloading files to directory: data sources/roads/\n",
      "Downloading files: 0% complete. 0 records processed.\n",
      "Downloading files: 10% complete. 75 records processed.\n",
      "Downloading files: 20% complete. 150 records processed.\n",
      "Downloading files: 30% complete. 225 records processed.\n",
      "Downloading files: 40% complete. 300 records processed.\n",
      "Downloading files: 50% complete. 375 records processed.\n",
      "Downloading files: 60% complete. 450 records processed.\n",
      "Downloading files: 70% complete. 525 records processed.\n",
      "Downloading files: 80% complete. 600 records processed.\n",
      "Downloading files: 90% complete. 675 records processed.\n",
      "Downloading files: 99% complete. 750 records processed.\n",
      "Downloading files: 100% complete. 753 records processed.\n",
      "Downloading files runtime: 0:41:27.478388\n",
      "Closing server connection.\n",
      "221 Goodbye.\n"
     ]
    }
   ],
   "source": [
    "### Obtain road data\n",
    "\n",
    "from ftplib import FTP\n",
    "import os\n",
    "\n",
    "# Set up FTP connection to data portal\n",
    "ftp_url = \"ftp2.census.gov\"\n",
    "roads_directory_ftp = \"/geo/tiger/TIGER2024/ROADS\"\n",
    "user = \"anonymous\"\n",
    "passwd = \"anonymous\"\n",
    "print(f\"Connecting to server: {ftp_url + roads_directory_ftp}\")\n",
    "ftp = FTP(\"ftp2.census.gov\")\n",
    "print(ftp.login(user=user, passwd=user))\n",
    "ftp.cwd(roads_directory_ftp)\n",
    "\n",
    "# Download files\n",
    "roads_directory = \"data sources/roads/\"\n",
    "files = ftp.nlst()\n",
    "\n",
    "# Skip any files already downloaded\n",
    "downloaded_files = os.listdir(roads_directory)\n",
    "downloaded_files = set(downloaded_files)\n",
    "files = [item for item in files if item not in downloaded_files]\n",
    "\n",
    "file_count = len(files)\n",
    "increment = set_increment(file_count, 10)\n",
    "progress_update = \"Downloading files\"\n",
    "print(f\"{progress_update} to directory: {roads_directory}\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Iterate through and download each file in roads directory\n",
    "for count, file_name in enumerate(files):\n",
    "    with open(roads_directory + file_name, 'wb') as file:\n",
    "        ftp.retrbinary(f\"RETR {file_name}\", file.write)\n",
    "    \n",
    "    update_progress(progress_update, count, file_count, increment)\n",
    "\n",
    "update_progress(progress_update, count, count, 1)\n",
    "end_time = datetime.now()\n",
    "runtime(progress_update, start_time, end_time)\n",
    "\n",
    "print(\"Closing server connection.\")\n",
    "print(ftp.quit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping files to directory: data sources/roads/\n",
      "Unzipping files: 0% complete. 0 records processed.\n",
      "Unzipping files: 10% complete. 323 records processed.\n",
      "Unzipping files: 20% complete. 646 records processed.\n",
      "Unzipping files: 30% complete. 969 records processed.\n",
      "Unzipping files: 40% complete. 1292 records processed.\n",
      "Unzipping files: 50% complete. 1615 records processed.\n",
      "Unzipping files: 60% complete. 1938 records processed.\n",
      "Unzipping files: 70% complete. 2261 records processed.\n",
      "Unzipping files: 80% complete. 2584 records processed.\n",
      "Unzipping files: 90% complete. 2907 records processed.\n",
      "Unzipping files: 100% complete. 3230 records processed.\n",
      "Unzipping files: 100% complete. 3232 records processed.\n",
      "Unzipping files runtime: 0:04:12.133783\n"
     ]
    }
   ],
   "source": [
    "### Unzip road files\n",
    "\n",
    "import zipfile\n",
    "\n",
    "zip_files = [f for f in os.listdir(roads_directory) if f.endswith(\".zip\")] # select only zips\n",
    "\n",
    "total_zips = len(zip_files)\n",
    "increment = set_increment(total_zips, 10)\n",
    "progress_update = \"Unzipping files\"\n",
    "print(f\"{progress_update} to directory: {roads_directory}\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "for count, zip_file in enumerate(zip_files):\n",
    "    zip_path = os.path.join(roads_directory, zip_file) # find filepath of zip\n",
    "    file_folder = os.path.join(roads_directory, os.path.splitext(zip_file)[0]) # create file folder\n",
    "    os.makedirs(file_folder, exist_ok=True) # create directory for extracted files from zip\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref: # extract zip\n",
    "        zip_ref.extractall(file_folder)\n",
    "    \n",
    "    update_progress(progress_update, count, total_zips, increment)\n",
    "\n",
    "update_progress(progress_update, count, count, 1)\n",
    "end_time = datetime.now()\n",
    "runtime(progress_update, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to server: ftp2.census.gov/geo/docs/reference/codes2020/national_county2020.txt\n",
      "230-Server: ftp2.census.gov\n",
      "230-\n",
      "230-Personal Identifiable Information (PII) shall not be placed on the FTP\n",
      "230-server without prior special arrangement and in conjunction with ITSO.\n",
      "230-\n",
      "230-NOTE: The data available for anonymous FTP download on this FTP server are\n",
      "230-also available over the Web:\n",
      "230-http://www2.census.gov\n",
      "230 Login successful.\n",
      "Downloading file to: data sources/national_county2020.txt\n",
      "Downloaded file.\n",
      "Closing server connection.\n",
      "221 Goodbye.\n"
     ]
    }
   ],
   "source": [
    "### Pull INCITS codes for U.S. counties\n",
    "\n",
    "# Set up FTP connection to data portal\n",
    "INCITS_directory_ftp = \"/geo/docs/reference/codes2020/\"\n",
    "INCITS_file = \"national_county2020.txt\"\n",
    "print(f\"Connecting to server: {ftp_url + INCITS_directory_ftp + INCITS_file}\")\n",
    "ftp = FTP(\"ftp2.census.gov\")\n",
    "print(ftp.login(user=user, passwd=user))\n",
    "\n",
    "# Download file\n",
    "INCITS_directory = 'data sources/'\n",
    "print(f\"Downloading file to: {INCITS_directory + INCITS_file}\")\n",
    "with open(INCITS_directory + INCITS_file, 'wb') as file:\n",
    "    ftp.retrbinary(f\"RETR {INCITS_directory_ftp + INCITS_file}\", file.write)\n",
    "print(\"Downloaded file.\")\n",
    "\n",
    "print(\"Closing server connection.\")\n",
    "print(ftp.quit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up street type statistics data structure\n",
    "# Note: road names are title-cased in input data\n",
    "\n",
    "# Get all primary street suffix names, title-cased\n",
    "suffixes_titlecased = {key.title(): value.title() for key, value in suffixes.items()}\n",
    "full_suffixes = set(suffixes_titlecased.values()) # filter unique names\n",
    "full_suffixes = list(full_suffixes)\n",
    "full_suffixes = sorted(full_suffixes)\n",
    "\n",
    "# Add additional street types (prefixes) not present on suffixes website, title-cased\n",
    "prefixes = {\n",
    "    \"Cr \"             : \"County Road\",\n",
    "    \"Cr-\"             : \"County Road\",\n",
    "    \"C R \"            : \"County Road\",\n",
    "    \"Co Rd \"          : \"County Road\",\n",
    "    \"County Road \"    : \"County Road\",\n",
    "    \"Co Hwy \"         : \"County Highway\",\n",
    "    \"County Highway \" : \"County Highway\",\n",
    "    \"Twp Rd \"         : \"Township Road\",\n",
    "    \"Township Road \"  : \"Township Road\",\n",
    "    \"FM \"             : \"Farm to Market Road\",\n",
    "    \"State Rte \"      : \"State Route\",\n",
    "    \"State Route \"    : \"State Route\",\n",
    "    \"State Rd \"       : \"State Road\",\n",
    "    \"State Road \"     : \"State Road\",\n",
    "    \"State Hwy \"      : \"State Highway\",\n",
    "    \"State Highway \"  : \"State Highway\",\n",
    "    \"Old Hwy \"        : \"Old Highway\",\n",
    "    \"Old US Hwy \"     : \"Old U.S. Highway\",\n",
    "    \"US Hwy \"         : \"U.S. Highway\",\n",
    "    \"I-\"              : \"Interstate\",\n",
    "    \"NFS Rd \"         : \"National Forest System Road\",\n",
    "    \"FS Rd \"          : \"Forest Service Road\",\n",
    "    \"BLM Rd \"         : \"Bureau of Land Management Road\"\n",
    "}\n",
    "full_prefixes = set(prefixes.values())\n",
    "full_prefixes = list(full_prefixes)\n",
    "full_prefixes = sorted(full_prefixes)\n",
    "\n",
    "street_types = full_suffixes + full_prefixes + [\"Other\", \"Unnamed\"] # combine street types & additional unsorted types\n",
    "st_stats_columns = [\"INCITS\", \"State\", \"County\"] + street_types # add identifier columns\n",
    "\n",
    "# Set up data structure for suffix statistics\n",
    "\n",
    "st_stats = pd.DataFrame(columns=st_stats_columns)\n",
    "# Load state abbreviations and county names from INCITS file\n",
    "INCITS_df = pd.read_csv(INCITS_directory + INCITS_file, dtype=str, delimiter=\"|\")\n",
    "st_stats[\"INCITS\"] = INCITS_df[\"STATEFP\"] + INCITS_df[\"COUNTYFP\"]\n",
    "st_stats[[\"State\", \"County\"]] = INCITS_df[[\"STATE\", \"COUNTYNAME\"]]\n",
    "# Title-case city-county features (independent cities), formatted as \"XYZ city\"\n",
    "st_stats[\"County\"] = st_stats[\"County\"].str.replace(\" city\", \" City\", regex=False)\n",
    "\n",
    "# Code to modify INCITS codes to replace CT counties with planning regions\n",
    "# Issue: CT data doesn't show on Looker Studio because it has CT counties geos instead of planning regions geos\n",
    "# https://www.federalregister.gov/documents/2022/06/06/2022-12063/change-to-county-equivalents-in-the-state-of-connecticut\n",
    "CT_INCITS_codes = [\n",
    "    [\"09110\", \"CT\", \"Capitol Planning Region\"],\n",
    "    [\"09120\", \"CT\", \"Greater Bridgeport Planning Region\"],\n",
    "    [\"09130\", \"CT\", \"Lower Connecticut River Valley Planning Region\"],\n",
    "    [\"09140\", \"CT\", \"Naugatuck Valley Planning Region\"],\n",
    "    [\"09150\", \"CT\", \"Northeastern Connecticut Planning Region\"],\n",
    "    [\"09160\", \"CT\", \"Northwest Hills Planning Region\"],\n",
    "    [\"09170\", \"CT\", \"South Central Connecticut Planning Region\"],\n",
    "    [\"09180\", \"CT\", \"Southeastern Connecticut Planning Region\"],\n",
    "    [\"09190\", \"CT\", \"Western Connecticut Planning Region\"]\n",
    "]\n",
    "CT_INCITS = pd.DataFrame(CT_INCITS_codes, columns=[\"INCITS\", \"State\", \"County\"])\n",
    "st_stats = st_stats[~st_stats[\"INCITS\"].str.startswith(\"09\")] # remove CT counties\n",
    "st_stats = pd.concat([st_stats, CT_INCITS]) # add CT planning regions\n",
    "\n",
    "st_stats[street_types] = 0.0 # set initial road length sums to 0\n",
    "st_stats = st_stats.set_index(\"INCITS\") # set index to INCITS codes\n",
    "st_stats = st_stats.sort_index() # re-sort by index to account for modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing roads: 0% complete. 0 records processed.\n",
      "Processing roads: 5% complete. 162 records processed.\n",
      "Processing roads: 10% complete. 324 records processed.\n",
      "Processing roads: 15% complete. 486 records processed.\n",
      "Processing roads: 20% complete. 648 records processed.\n",
      "Processing roads: 25% complete. 810 records processed.\n",
      "Processing roads: 30% complete. 972 records processed.\n",
      "Processing roads: 35% complete. 1134 records processed.\n",
      "Processing roads: 40% complete. 1296 records processed.\n",
      "Processing roads: 45% complete. 1458 records processed.\n",
      "Processing roads: 50% complete. 1620 records processed.\n",
      "Processing roads: 55% complete. 1782 records processed.\n",
      "Processing roads: 60% complete. 1944 records processed.\n",
      "Processing roads: 65% complete. 2106 records processed.\n",
      "Processing roads: 70% complete. 2268 records processed.\n",
      "Processing roads: 75% complete. 2430 records processed.\n",
      "Processing roads: 80% complete. 2592 records processed.\n",
      "Processing roads: 85% complete. 2754 records processed.\n",
      "Processing roads: 90% complete. 2916 records processed.\n",
      "Processing roads: 95% complete. 3078 records processed.\n",
      "Processing roads: 100% complete. 3232 records processed.\n",
      "Processing roads runtime: 0:14:26.069748\n",
      "Total roads processed: 17865320\n",
      "Exporting statistics to directory: data outputs/street_type_statistics.csv\n"
     ]
    }
   ],
   "source": [
    "### Process road data\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "# Helper function for street type calculations\n",
    "def find_street_type(street_name, prefixes, suffixes):\n",
    "    \"\"\"\n",
    "    Finds and returns the street type of a street name if the type exists in either the prefix or suffix inputs.\n",
    "\n",
    "    :param street_name: The street name to find the street type of.\n",
    "    :param prefixes: A dict of prefixes, where the key is the abbrevation, and the value is the full prefix.\n",
    "    :param suffixes: A dict of suffixes, where the key is the abbreviation, and the avlue is the full prefix.\n",
    "    :return: The street type of the street name.\n",
    "    \"\"\"\n",
    "    # Check if street name exists\n",
    "    if street_name is None:\n",
    "        return \"Unnamed\"\n",
    "    else:\n",
    "        # Strip ordinal directions from start of string\n",
    "        ordinals = [\"N \", \"S \", \"E \", \"W \", \"NE \", \"NW \", \"SE \", \"SW \"]\n",
    "        for ordinal in ordinals:\n",
    "            if street_name.startswith(ordinal):\n",
    "                street_name = street_name[len(ordinal):]\n",
    "                break # Only one ordinal direction per name, so multiple strips are unnecessary\n",
    "\n",
    "        # Check for prefixes\n",
    "        for prefix in prefixes.keys():\n",
    "            if street_name.startswith(prefix):\n",
    "                return prefixes[prefix]\n",
    "\n",
    "        # Check for suffixes\n",
    "        name_tuples = street_name.split(\" \") # street name components are separated by spaces\n",
    "        for name_tuple in reversed(name_tuples): # reverse order, as suffixes are at the end\n",
    "            for suffix in suffixes.keys():\n",
    "                if name_tuple == suffix:\n",
    "                    return suffixes[name_tuple]\n",
    "\n",
    "        # No listed street types were found\n",
    "        return \"Other\"\n",
    "\n",
    "# Iterate through each road file's folder\n",
    "subfolders = [folder for folder in os.scandir(roads_directory) if folder.is_dir()]\n",
    "\n",
    "total_folders = len(subfolders)\n",
    "increment = set_increment(total_folders, 5)\n",
    "progress_update = \"Processing roads\"\n",
    "start_time = datetime.now()\n",
    "roads = 0 # tracking how many roads were processed\n",
    "\n",
    "for count, subfolder in enumerate(subfolders):\n",
    "    # Obtain road dataset file path\n",
    "    road_path = os.path.join(subfolder.path, subfolder.name + \".shp\")\n",
    "    road_path = os.path.normpath(road_path)\n",
    "\n",
    "    if os.path.exists(road_path):\n",
    "        # Dataset setup                         # 0123456789012345678\n",
    "        # Obtain INCITS code; each file is named \"tl_1234_12345_roads\"\n",
    "        INCITS_code = subfolder.name[8:13]\n",
    "        \n",
    "        # Load road dataset\n",
    "        gdf = gpd.read_file(road_path)\n",
    "        # Calculate the length (in miles) of each road segment\n",
    "        gdf = gdf.to_crs(9311) # re-project to PCS\n",
    "        gdf[\"LENGTH\"] = gdf.geometry.length * 0.0006213711 # convert meters to miles\n",
    "\n",
    "        # Identify street type\n",
    "        gdf[\"STREETTYPE\"] = [find_street_type(name, prefixes, suffixes_titlecased) for name in gdf[\"FULLNAME\"]]\n",
    "\n",
    "        # Sum and store street type lengths\n",
    "        summed_lengths = gdf.groupby(\"STREETTYPE\")[\"LENGTH\"].sum() # sum lengths\n",
    "        st_stats.loc[INCITS_code, summed_lengths.index] = summed_lengths # replace values only if street type is present\n",
    "\n",
    "        roads += len(gdf.index)\n",
    "\n",
    "    update_progress(progress_update, count, total_folders, increment)\n",
    "\n",
    "update_progress(progress_update, count, count, 1)\n",
    "end_time = datetime.now()\n",
    "runtime(progress_update, start_time, end_time)\n",
    "print(f\"Total roads processed: {roads}\")\n",
    "\n",
    "# Export statistics\n",
    "output_file = \"data outputs/street_type_statistics.csv\"\n",
    "print(f\"Exporting statistics to directory: {output_file}\")\n",
    "st_stats.to_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting trimmed statistics to directory: data outputs/street_statistics_trimmed.csv\n",
      "Exporting summed statistics to directory: data outputs/street_statistics_summed.csv\n"
     ]
    }
   ],
   "source": [
    "### Reformat data for dashboard use\n",
    "# Note: Looker Studio has a column limit of 200; the initial street type statistics has 225\n",
    "\n",
    "# Trim street type statistics to include only the most common street types\n",
    "max_cols = 100 # maximum number of street types to include\n",
    "exclusions = [\"State\", \"County\", \"Other\", \"Unnamed\"] # exclude columns from sumnation analysis\n",
    "st_lengths = st_stats.drop(columns=exclusions)\n",
    "st_sums = st_lengths.sum()\n",
    "top_sts = st_sums.nlargest(max_cols).index # find the most common street types by length\n",
    "st_stats_trimmed = st_stats[exclusions + list(top_sts)] # trim statistics\n",
    "st_stats_trimmed = st_stats_trimmed.copy() # make a copy of the original DF, instead of a slice\n",
    "\n",
    "# Add least common street types to \"Other\" length\n",
    "bottom_sts = st_sums.index.difference(top_sts)\n",
    "st_stats_trimmed.loc[:, \"Other\"] += st_stats[bottom_sts].sum(axis=1)\n",
    "\n",
    "# Find and append the top street type for each county\n",
    "top_sts_county = st_lengths.idxmax(axis=1)\n",
    "st_stats_trimmed.loc[:, \"Top Street Type\"] = top_sts_county\n",
    "\n",
    "# Exclude county data not from states\n",
    "non_states = [\"60\", \"66\", \"69\", \"72\", \"74\", \"78\"]\n",
    "st_stats_trimmed = st_stats_trimmed[~st_stats_trimmed.index.str.startswith(tuple(non_states))]\n",
    "\n",
    "# Export statistics\n",
    "output_file = \"data outputs/street_statistics_trimmed.csv\"\n",
    "print(f\"Exporting trimmed statistics to directory: {output_file}\")\n",
    "st_stats_trimmed.to_csv(output_file)\n",
    "\n",
    "# Reshape statistics to sum up lengths for each street type across the country\n",
    "st_stats_summed = st_stats_trimmed.drop(columns=[\"State\", \"County\", \"Top Street Type\"])\n",
    "st_stats_summed = st_stats_summed.drop(columns=\"Unnamed\") # drop statistics for unnamed roads\n",
    "st_stats_summed = st_stats_summed.melt(var_name=\"Street Type\", value_name=\"Length\")\n",
    "st_stats_summed = st_stats_summed.groupby(\"Street Type\").sum().reset_index()\n",
    "\n",
    "# Select the top 15 street type lengths (excluding \"Other\") and combine the rest into a new \"Other\" row\n",
    "st_no_other = st_stats_summed[st_stats_summed[\"Street Type\"] != \"Other\"]\n",
    "top_15_sts = st_no_other.nlargest(15, \"Length\")\n",
    "other_rows = st_no_other[~st_no_other[\"Street Type\"].isin(top_15_sts[\"Street Type\"])]\n",
    "st_stats_summed_other = st_stats_summed[st_stats_summed[\"Street Type\"] == \"Other\"].values[0][1]\n",
    "other_row = pd.DataFrame({\n",
    "    \"Street Type\": [\"Other\"],\n",
    "    \"Length\": [other_rows[\"Length\"].sum() + st_stats_summed_other]\n",
    "})\n",
    "top_15_sts_plus_other = pd.concat([top_15_sts, other_row], ignore_index=True)\n",
    "\n",
    "# Export sums file\n",
    "output_file = \"data outputs/street_statistics_summed.csv\"\n",
    "print(f\"Exporting summed statistics to directory: {output_file}\")\n",
    "top_15_sts_plus_other.to_csv(output_file, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
